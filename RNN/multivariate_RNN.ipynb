{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq1 = array([x for x in range(0,100,10)])\n",
    "in_seq2 = array([x for x in range(5,105,10)])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5],\n",
       "       [15],\n",
       "       [25],\n",
       "       [35],\n",
       "       [45],\n",
       "       [55],\n",
       "       [65],\n",
       "       [75],\n",
       "       [85],\n",
       "       [95]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MV_LSTM(torch.nn.Module):\n",
    "    def __init__(self,n_features,seq_length):\n",
    "        super(MV_LSTM, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.seq_len = seq_length\n",
    "        self.n_hidden = 20 # number of hidden states\n",
    "        self.n_layers = 1 # number of LSTM layers (stacked)\n",
    "    \n",
    "        self.l_lstm = torch.nn.LSTM(input_size = n_features, \n",
    "                                 hidden_size = self.n_hidden,\n",
    "                                 num_layers = self.n_layers, \n",
    "                                 batch_first = True)\n",
    "        # according to pytorch docs LSTM output is \n",
    "        # (batch_size,seq_len, num_directions * hidden_size)\n",
    "        # when considering batch_first = True\n",
    "        self.l_linear = torch.nn.Linear(self.n_hidden*self.seq_len, 1)\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
    "        self.hidden = (hidden_state, cell_state)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):        \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
    "        # lstm_out(with batch_first = True) is \n",
    "        # (batch_size,seq_len,num_directions * hidden_size)\n",
    "        # for following linear layer we want to keep batch_size dimension and merge rest       \n",
    "        # .contiguous() -> solves tensor compatibility error\n",
    "        x = lstm_out.contiguous().view(batch_size,-1)\n",
    "        return self.l_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 3, 2) (8,)\n"
     ]
    }
   ],
   "source": [
    "n_features = 2 # this is number of parallel inputs\n",
    "n_timesteps = 3 # this is number of timesteps\n",
    "\n",
    "# convert dataset into input/output\n",
    "X, y = split_sequences(dataset, n_timesteps)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# create NN\n",
    "mv_net = MV_LSTM(n_features,n_timesteps)\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-1)\n",
    "\n",
    "train_episodes = 500\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "step :  0 loss :  0.5828284621238708\n",
      "8\n",
      "step :  1 loss :  0.5828284621238708\n",
      "8\n",
      "step :  2 loss :  0.5828284621238708\n",
      "8\n",
      "step :  3 loss :  0.5828284621238708\n",
      "8\n",
      "step :  4 loss :  0.5828284621238708\n",
      "8\n",
      "step :  5 loss :  0.5828284621238708\n",
      "8\n",
      "step :  6 loss :  0.5828284621238708\n",
      "8\n",
      "step :  7 loss :  0.5828284621238708\n",
      "8\n",
      "step :  8 loss :  0.5828284621238708\n",
      "8\n",
      "step :  9 loss :  0.5828284621238708\n",
      "8\n",
      "step :  10 loss :  0.5828284621238708\n",
      "8\n",
      "step :  11 loss :  0.5828284621238708\n",
      "8\n",
      "step :  12 loss :  0.5828284621238708\n",
      "8\n",
      "step :  13 loss :  0.5828284621238708\n",
      "8\n",
      "step :  14 loss :  0.5828284621238708\n",
      "8\n",
      "step :  15 loss :  0.5828284621238708\n",
      "8\n",
      "step :  16 loss :  0.5828284621238708\n",
      "8\n",
      "step :  17 loss :  0.5828284621238708\n",
      "8\n",
      "step :  18 loss :  0.5828284621238708\n",
      "8\n",
      "step :  19 loss :  0.5828284621238708\n",
      "8\n",
      "step :  20 loss :  0.5828284621238708\n",
      "8\n",
      "step :  21 loss :  0.5828284621238708\n",
      "8\n",
      "step :  22 loss :  0.5828284621238708\n",
      "8\n",
      "step :  23 loss :  0.5828284621238708\n",
      "8\n",
      "step :  24 loss :  0.5828284621238708\n",
      "8\n",
      "step :  25 loss :  0.5828284621238708\n",
      "8\n",
      "step :  26 loss :  0.5828284621238708\n",
      "8\n",
      "step :  27 loss :  0.5828284621238708\n",
      "8\n",
      "step :  28 loss :  0.5828284621238708\n",
      "8\n",
      "step :  29 loss :  0.5828284621238708\n",
      "8\n",
      "step :  30 loss :  0.5828284621238708\n",
      "8\n",
      "step :  31 loss :  0.5828284621238708\n",
      "8\n",
      "step :  32 loss :  0.5828284621238708\n",
      "8\n",
      "step :  33 loss :  0.5828284621238708\n",
      "8\n",
      "step :  34 loss :  0.5828284621238708\n",
      "8\n",
      "step :  35 loss :  0.5828284621238708\n",
      "8\n",
      "step :  36 loss :  0.5828284621238708\n",
      "8\n",
      "step :  37 loss :  0.5828284621238708\n",
      "8\n",
      "step :  38 loss :  0.5828284621238708\n",
      "8\n",
      "step :  39 loss :  0.5828284621238708\n",
      "8\n",
      "step :  40 loss :  0.5828284621238708\n",
      "8\n",
      "step :  41 loss :  0.5828284621238708\n",
      "8\n",
      "step :  42 loss :  0.5828284621238708\n",
      "8\n",
      "step :  43 loss :  0.5828284621238708\n",
      "8\n",
      "step :  44 loss :  0.5828284621238708\n",
      "8\n",
      "step :  45 loss :  0.5828284621238708\n",
      "8\n",
      "step :  46 loss :  0.5828284621238708\n",
      "8\n",
      "step :  47 loss :  0.5828284621238708\n",
      "8\n",
      "step :  48 loss :  0.5828284621238708\n",
      "8\n",
      "step :  49 loss :  0.5828284621238708\n",
      "8\n",
      "step :  50 loss :  0.5828284621238708\n",
      "8\n",
      "step :  51 loss :  0.5828284621238708\n",
      "8\n",
      "step :  52 loss :  0.5828284621238708\n",
      "8\n",
      "step :  53 loss :  0.5828284621238708\n",
      "8\n",
      "step :  54 loss :  0.5828284621238708\n",
      "8\n",
      "step :  55 loss :  0.5828284621238708\n",
      "8\n",
      "step :  56 loss :  0.5828284621238708\n",
      "8\n",
      "step :  57 loss :  0.5828284621238708\n",
      "8\n",
      "step :  58 loss :  0.5828284621238708\n",
      "8\n",
      "step :  59 loss :  0.5828284621238708\n",
      "8\n",
      "step :  60 loss :  0.5828284621238708\n",
      "8\n",
      "step :  61 loss :  0.5828284621238708\n",
      "8\n",
      "step :  62 loss :  0.5828284621238708\n",
      "8\n",
      "step :  63 loss :  0.5828284621238708\n",
      "8\n",
      "step :  64 loss :  0.5828284621238708\n",
      "8\n",
      "step :  65 loss :  0.5828284621238708\n",
      "8\n",
      "step :  66 loss :  0.5828284621238708\n",
      "8\n",
      "step :  67 loss :  0.5828284621238708\n",
      "8\n",
      "step :  68 loss :  0.5828284621238708\n",
      "8\n",
      "step :  69 loss :  0.5828284621238708\n",
      "8\n",
      "step :  70 loss :  0.5828284621238708\n",
      "8\n",
      "step :  71 loss :  0.5828284621238708\n",
      "8\n",
      "step :  72 loss :  0.5828284621238708\n",
      "8\n",
      "step :  73 loss :  0.5828284621238708\n",
      "8\n",
      "step :  74 loss :  0.5828284621238708\n",
      "8\n",
      "step :  75 loss :  0.5828284621238708\n",
      "8\n",
      "step :  76 loss :  0.5828284621238708\n",
      "8\n",
      "step :  77 loss :  0.5828284621238708\n",
      "8\n",
      "step :  78 loss :  0.5828284621238708\n",
      "8\n",
      "step :  79 loss :  0.5828284621238708\n",
      "8\n",
      "step :  80 loss :  0.5828284621238708\n",
      "8\n",
      "step :  81 loss :  0.5828284621238708\n",
      "8\n",
      "step :  82 loss :  0.5828284621238708\n",
      "8\n",
      "step :  83 loss :  0.5828284621238708\n",
      "8\n",
      "step :  84 loss :  0.5828284621238708\n",
      "8\n",
      "step :  85 loss :  0.5828284621238708\n",
      "8\n",
      "step :  86 loss :  0.5828284621238708\n",
      "8\n",
      "step :  87 loss :  0.5828284621238708\n",
      "8\n",
      "step :  88 loss :  0.5828284621238708\n",
      "8\n",
      "step :  89 loss :  0.5828284621238708\n",
      "8\n",
      "step :  90 loss :  0.5828284621238708\n",
      "8\n",
      "step :  91 loss :  0.5828284621238708\n",
      "8\n",
      "step :  92 loss :  0.5828284621238708\n",
      "8\n",
      "step :  93 loss :  0.5828284621238708\n",
      "8\n",
      "step :  94 loss :  0.5828284621238708\n",
      "8\n",
      "step :  95 loss :  0.5828284621238708\n",
      "8\n",
      "step :  96 loss :  0.5828284621238708\n",
      "8\n",
      "step :  97 loss :  0.5828284621238708\n",
      "8\n",
      "step :  98 loss :  0.5828284621238708\n",
      "8\n",
      "step :  99 loss :  0.5828284621238708\n",
      "8\n",
      "step :  100 loss :  0.5828284621238708\n",
      "8\n",
      "step :  101 loss :  0.5828284621238708\n",
      "8\n",
      "step :  102 loss :  0.5828284621238708\n",
      "8\n",
      "step :  103 loss :  0.5828284621238708\n",
      "8\n",
      "step :  104 loss :  0.5828284621238708\n",
      "8\n",
      "step :  105 loss :  0.5828284621238708\n",
      "8\n",
      "step :  106 loss :  0.5828284621238708\n",
      "8\n",
      "step :  107 loss :  0.5828284621238708\n",
      "8\n",
      "step :  108 loss :  0.5828284621238708\n",
      "8\n",
      "step :  109 loss :  0.5828284621238708\n",
      "8\n",
      "step :  110 loss :  0.5828284621238708\n",
      "8\n",
      "step :  111 loss :  0.5828284621238708\n",
      "8\n",
      "step :  112 loss :  0.5828284621238708\n",
      "8\n",
      "step :  113 loss :  0.5828284621238708\n",
      "8\n",
      "step :  114 loss :  0.5828284621238708\n",
      "8\n",
      "step :  115 loss :  0.5828284621238708\n",
      "8\n",
      "step :  116 loss :  0.5828284621238708\n",
      "8\n",
      "step :  117 loss :  0.5828284621238708\n",
      "8\n",
      "step :  118 loss :  0.5828284621238708\n",
      "8\n",
      "step :  119 loss :  0.5828284621238708\n",
      "8\n",
      "step :  120 loss :  0.5828284621238708\n",
      "8\n",
      "step :  121 loss :  0.5828284621238708\n",
      "8\n",
      "step :  122 loss :  0.5828284621238708\n",
      "8\n",
      "step :  123 loss :  0.5828284621238708\n",
      "8\n",
      "step :  124 loss :  0.5828284621238708\n",
      "8\n",
      "step :  125 loss :  0.5828284621238708\n",
      "8\n",
      "step :  126 loss :  0.5828284621238708\n",
      "8\n",
      "step :  127 loss :  0.5828284621238708\n",
      "8\n",
      "step :  128 loss :  0.5828284621238708\n",
      "8\n",
      "step :  129 loss :  0.5828284621238708\n",
      "8\n",
      "step :  130 loss :  0.5828284621238708\n",
      "8\n",
      "step :  131 loss :  0.5828284621238708\n",
      "8\n",
      "step :  132 loss :  0.5828284621238708\n",
      "8\n",
      "step :  133 loss :  0.5828284621238708\n",
      "8\n",
      "step :  134 loss :  0.5828284621238708\n",
      "8\n",
      "step :  135 loss :  0.5828284621238708\n",
      "8\n",
      "step :  136 loss :  0.5828284621238708\n",
      "8\n",
      "step :  137 loss :  0.5828284621238708\n",
      "8\n",
      "step :  138 loss :  0.5828284621238708\n",
      "8\n",
      "step :  139 loss :  0.5828284621238708\n",
      "8\n",
      "step :  140 loss :  0.5828284621238708\n",
      "8\n",
      "step :  141 loss :  0.5828284621238708\n",
      "8\n",
      "step :  142 loss :  0.5828284621238708\n",
      "8\n",
      "step :  143 loss :  0.5828284621238708\n",
      "8\n",
      "step :  144 loss :  0.5828284621238708\n",
      "8\n",
      "step :  145 loss :  0.5828284621238708\n",
      "8\n",
      "step :  146 loss :  0.5828284621238708\n",
      "8\n",
      "step :  147 loss :  0.5828284621238708\n",
      "8\n",
      "step :  148 loss :  0.5828284621238708\n",
      "8\n",
      "step :  149 loss :  0.5828284621238708\n",
      "8\n",
      "step :  150 loss :  0.5828284621238708\n",
      "8\n",
      "step :  151 loss :  0.5828284621238708\n",
      "8\n",
      "step :  152 loss :  0.5828284621238708\n",
      "8\n",
      "step :  153 loss :  0.5828284621238708\n",
      "8\n",
      "step :  154 loss :  0.5828284621238708\n",
      "8\n",
      "step :  155 loss :  0.5828284621238708\n",
      "8\n",
      "step :  156 loss :  0.5828284621238708\n",
      "8\n",
      "step :  157 loss :  0.5828284621238708\n",
      "8\n",
      "step :  158 loss :  0.5828284621238708\n",
      "8\n",
      "step :  159 loss :  0.5828284621238708\n",
      "8\n",
      "step :  160 loss :  0.5828284621238708\n",
      "8\n",
      "step :  161 loss :  0.5828284621238708\n",
      "8\n",
      "step :  162 loss :  0.5828284621238708\n",
      "8\n",
      "step :  163 loss :  0.5828284621238708\n",
      "8\n",
      "step :  164 loss :  0.5828284621238708\n",
      "8\n",
      "step :  165 loss :  0.5828284621238708\n",
      "8\n",
      "step :  166 loss :  0.5828284621238708\n",
      "8\n",
      "step :  167 loss :  0.5828284621238708\n",
      "8\n",
      "step :  168 loss :  0.5828284621238708\n",
      "8\n",
      "step :  169 loss :  0.5828284621238708\n",
      "8\n",
      "step :  170 loss :  0.5828284621238708\n",
      "8\n",
      "step :  171 loss :  0.5828284621238708\n",
      "8\n",
      "step :  172 loss :  0.5828284621238708\n",
      "8\n",
      "step :  173 loss :  0.5828284621238708\n",
      "8\n",
      "step :  174 loss :  0.5828284621238708\n",
      "8\n",
      "step :  175 loss :  0.5828284621238708\n",
      "8\n",
      "step :  176 loss :  0.5828284621238708\n",
      "8\n",
      "step :  177 loss :  0.5828284621238708\n",
      "8\n",
      "step :  178 loss :  0.5828284621238708\n",
      "8\n",
      "step :  179 loss :  0.5828284621238708\n",
      "8\n",
      "step :  180 loss :  0.5828284621238708\n",
      "8\n",
      "step :  181 loss :  0.5828284621238708\n",
      "8\n",
      "step :  182 loss :  0.5828284621238708\n",
      "8\n",
      "step :  183 loss :  0.5828284621238708\n",
      "8\n",
      "step :  184 loss :  0.5828284621238708\n",
      "8\n",
      "step :  185 loss :  0.5828284621238708\n",
      "8\n",
      "step :  186 loss :  0.5828284621238708\n",
      "8\n",
      "step :  187 loss :  0.5828284621238708\n",
      "8\n",
      "step :  188 loss :  0.5828284621238708\n",
      "8\n",
      "step :  189 loss :  0.5828284621238708\n",
      "8\n",
      "step :  190 loss :  0.5828284621238708\n",
      "8\n",
      "step :  191 loss :  0.5828284621238708\n",
      "8\n",
      "step :  192 loss :  0.5828284621238708\n",
      "8\n",
      "step :  193 loss :  0.5828284621238708\n",
      "8\n",
      "step :  194 loss :  0.5828284621238708\n",
      "8\n",
      "step :  195 loss :  0.5828284621238708\n",
      "8\n",
      "step :  196 loss :  0.5828284621238708\n",
      "8\n",
      "step :  197 loss :  0.5828284621238708\n",
      "8\n",
      "step :  198 loss :  0.5828284621238708\n",
      "8\n",
      "step :  199 loss :  0.5828284621238708\n",
      "8\n",
      "step :  200 loss :  0.5828284621238708\n",
      "8\n",
      "step :  201 loss :  0.5828284621238708\n",
      "8\n",
      "step :  202 loss :  0.5828284621238708\n",
      "8\n",
      "step :  203 loss :  0.5828284621238708\n",
      "8\n",
      "step :  204 loss :  0.5828284621238708\n",
      "8\n",
      "step :  205 loss :  0.5828284621238708\n",
      "8\n",
      "step :  206 loss :  0.5828284621238708\n",
      "8\n",
      "step :  207 loss :  0.5828284621238708\n",
      "8\n",
      "step :  208 loss :  0.5828284621238708\n",
      "8\n",
      "step :  209 loss :  0.5828284621238708\n",
      "8\n",
      "step :  210 loss :  0.5828284621238708\n",
      "8\n",
      "step :  211 loss :  0.5828284621238708\n",
      "8\n",
      "step :  212 loss :  0.5828284621238708\n",
      "8\n",
      "step :  213 loss :  0.5828284621238708\n",
      "8\n",
      "step :  214 loss :  0.5828284621238708\n",
      "8\n",
      "step :  215 loss :  0.5828284621238708\n",
      "8\n",
      "step :  216 loss :  0.5828284621238708\n",
      "8\n",
      "step :  217 loss :  0.5828284621238708\n",
      "8\n",
      "step :  218 loss :  0.5828284621238708\n",
      "8\n",
      "step :  219 loss :  0.5828284621238708\n",
      "8\n",
      "step :  220 loss :  0.5828284621238708\n",
      "8\n",
      "step :  221 loss :  0.5828284621238708\n",
      "8\n",
      "step :  222 loss :  0.5828284621238708\n",
      "8\n",
      "step :  223 loss :  0.5828284621238708\n",
      "8\n",
      "step :  224 loss :  0.5828284621238708\n",
      "8\n",
      "step :  225 loss :  0.5828284621238708\n",
      "8\n",
      "step :  226 loss :  0.5828284621238708\n",
      "8\n",
      "step :  227 loss :  0.5828284621238708\n",
      "8\n",
      "step :  228 loss :  0.5828284621238708\n",
      "8\n",
      "step :  229 loss :  0.5828284621238708\n",
      "8\n",
      "step :  230 loss :  0.5828284621238708\n",
      "8\n",
      "step :  231 loss :  0.5828284621238708\n",
      "8\n",
      "step :  232 loss :  0.5828284621238708\n",
      "8\n",
      "step :  233 loss :  0.5828284621238708\n",
      "8\n",
      "step :  234 loss :  0.5828284621238708\n",
      "8\n",
      "step :  235 loss :  0.5828284621238708\n",
      "8\n",
      "step :  236 loss :  0.5828284621238708\n",
      "8\n",
      "step :  237 loss :  0.5828284621238708\n",
      "8\n",
      "step :  238 loss :  0.5828284621238708\n",
      "8\n",
      "step :  239 loss :  0.5828284621238708\n",
      "8\n",
      "step :  240 loss :  0.5828284621238708\n",
      "8\n",
      "step :  241 loss :  0.5828284621238708\n",
      "8\n",
      "step :  242 loss :  0.5828284621238708\n",
      "8\n",
      "step :  243 loss :  0.5828284621238708\n",
      "8\n",
      "step :  244 loss :  0.5828284621238708\n",
      "8\n",
      "step :  245 loss :  0.5828284621238708\n",
      "8\n",
      "step :  246 loss :  0.5828284621238708\n",
      "8\n",
      "step :  247 loss :  0.5828284621238708\n",
      "8\n",
      "step :  248 loss :  0.5828284621238708\n",
      "8\n",
      "step :  249 loss :  0.5828284621238708\n",
      "8\n",
      "step :  250 loss :  0.5828284621238708\n",
      "8\n",
      "step :  251 loss :  0.5828284621238708\n",
      "8\n",
      "step :  252 loss :  0.5828284621238708\n",
      "8\n",
      "step :  253 loss :  0.5828284621238708\n",
      "8\n",
      "step :  254 loss :  0.5828284621238708\n",
      "8\n",
      "step :  255 loss :  0.5828284621238708\n",
      "8\n",
      "step :  256 loss :  0.5828284621238708\n",
      "8\n",
      "step :  257 loss :  0.5828284621238708\n",
      "8\n",
      "step :  258 loss :  0.5828284621238708\n",
      "8\n",
      "step :  259 loss :  0.5828284621238708\n",
      "8\n",
      "step :  260 loss :  0.5828284621238708\n",
      "8\n",
      "step :  261 loss :  0.5828284621238708\n",
      "8\n",
      "step :  262 loss :  0.5828284621238708\n",
      "8\n",
      "step :  263 loss :  0.5828284621238708\n",
      "8\n",
      "step :  264 loss :  0.5828284621238708\n",
      "8\n",
      "step :  265 loss :  0.5828284621238708\n",
      "8\n",
      "step :  266 loss :  0.5828284621238708\n",
      "8\n",
      "step :  267 loss :  0.5828284621238708\n",
      "8\n",
      "step :  268 loss :  0.5828284621238708\n",
      "8\n",
      "step :  269 loss :  0.5828284621238708\n",
      "8\n",
      "step :  270 loss :  0.5828284621238708\n",
      "8\n",
      "step :  271 loss :  0.5828284621238708\n",
      "8\n",
      "step :  272 loss :  0.5828284621238708\n",
      "8\n",
      "step :  273 loss :  0.5828284621238708\n",
      "8\n",
      "step :  274 loss :  0.5828284621238708\n",
      "8\n",
      "step :  275 loss :  0.5828284621238708\n",
      "8\n",
      "step :  276 loss :  0.5828284621238708\n",
      "8\n",
      "step :  277 loss :  0.5828284621238708\n",
      "8\n",
      "step :  278 loss :  0.5828284621238708\n",
      "8\n",
      "step :  279 loss :  0.5828284621238708\n",
      "8\n",
      "step :  280 loss :  0.5828284621238708\n",
      "8\n",
      "step :  281 loss :  0.5828284621238708\n",
      "8\n",
      "step :  282 loss :  0.5828284621238708\n",
      "8\n",
      "step :  283 loss :  0.5828284621238708\n",
      "8\n",
      "step :  284 loss :  0.5828284621238708\n",
      "8\n",
      "step :  285 loss :  0.5828284621238708\n",
      "8\n",
      "step :  286 loss :  0.5828284621238708\n",
      "8\n",
      "step :  287 loss :  0.5828284621238708\n",
      "8\n",
      "step :  288 loss :  0.5828284621238708\n",
      "8\n",
      "step :  289 loss :  0.5828284621238708\n",
      "8\n",
      "step :  290 loss :  0.5828284621238708\n",
      "8\n",
      "step :  291 loss :  0.5828284621238708\n",
      "8\n",
      "step :  292 loss :  0.5828284621238708\n",
      "8\n",
      "step :  293 loss :  0.5828284621238708\n",
      "8\n",
      "step :  294 loss :  0.5828284621238708\n",
      "8\n",
      "step :  295 loss :  0.5828284621238708\n",
      "8\n",
      "step :  296 loss :  0.5828284621238708\n",
      "8\n",
      "step :  297 loss :  0.5828284621238708\n",
      "8\n",
      "step :  298 loss :  0.5828284621238708\n",
      "8\n",
      "step :  299 loss :  0.5828284621238708\n",
      "8\n",
      "step :  300 loss :  0.5828284621238708\n",
      "8\n",
      "step :  301 loss :  0.5828284621238708\n",
      "8\n",
      "step :  302 loss :  0.5828284621238708\n",
      "8\n",
      "step :  303 loss :  0.5828284621238708\n",
      "8\n",
      "step :  304 loss :  0.5828284621238708\n",
      "8\n",
      "step :  305 loss :  0.5828284621238708\n",
      "8\n",
      "step :  306 loss :  0.5828284621238708\n",
      "8\n",
      "step :  307 loss :  0.5828284621238708\n",
      "8\n",
      "step :  308 loss :  0.5828284621238708\n",
      "8\n",
      "step :  309 loss :  0.5828284621238708\n",
      "8\n",
      "step :  310 loss :  0.5828284621238708\n",
      "8\n",
      "step :  311 loss :  0.5828284621238708\n",
      "8\n",
      "step :  312 loss :  0.5828284621238708\n",
      "8\n",
      "step :  313 loss :  0.5828284621238708\n",
      "8\n",
      "step :  314 loss :  0.5828284621238708\n",
      "8\n",
      "step :  315 loss :  0.5828284621238708\n",
      "8\n",
      "step :  316 loss :  0.5828284621238708\n",
      "8\n",
      "step :  317 loss :  0.5828284621238708\n",
      "8\n",
      "step :  318 loss :  0.5828284621238708\n",
      "8\n",
      "step :  319 loss :  0.5828284621238708\n",
      "8\n",
      "step :  320 loss :  0.5828284621238708\n",
      "8\n",
      "step :  321 loss :  0.5828284621238708\n",
      "8\n",
      "step :  322 loss :  0.5828284621238708\n",
      "8\n",
      "step :  323 loss :  0.5828284621238708\n",
      "8\n",
      "step :  324 loss :  0.5828284621238708\n",
      "8\n",
      "step :  325 loss :  0.5828284621238708\n",
      "8\n",
      "step :  326 loss :  0.5828284621238708\n",
      "8\n",
      "step :  327 loss :  0.5828284621238708\n",
      "8\n",
      "step :  328 loss :  0.5828284621238708\n",
      "8\n",
      "step :  329 loss :  0.5828284621238708\n",
      "8\n",
      "step :  330 loss :  0.5828284621238708\n",
      "8\n",
      "step :  331 loss :  0.5828284621238708\n",
      "8\n",
      "step :  332 loss :  0.5828284621238708\n",
      "8\n",
      "step :  333 loss :  0.5828284621238708\n",
      "8\n",
      "step :  334 loss :  0.5828284621238708\n",
      "8\n",
      "step :  335 loss :  0.5828284621238708\n",
      "8\n",
      "step :  336 loss :  0.5828284621238708\n",
      "8\n",
      "step :  337 loss :  0.5828284621238708\n",
      "8\n",
      "step :  338 loss :  0.5828284621238708\n",
      "8\n",
      "step :  339 loss :  0.5828284621238708\n",
      "8\n",
      "step :  340 loss :  0.5828284621238708\n",
      "8\n",
      "step :  341 loss :  0.5828284621238708\n",
      "8\n",
      "step :  342 loss :  0.5828284621238708\n",
      "8\n",
      "step :  343 loss :  0.5828284621238708\n",
      "8\n",
      "step :  344 loss :  0.5828284621238708\n",
      "8\n",
      "step :  345 loss :  0.5828284621238708\n",
      "8\n",
      "step :  346 loss :  0.5828284621238708\n",
      "8\n",
      "step :  347 loss :  0.5828284621238708\n",
      "8\n",
      "step :  348 loss :  0.5828284621238708\n",
      "8\n",
      "step :  349 loss :  0.5828284621238708\n",
      "8\n",
      "step :  350 loss :  0.5828284621238708\n",
      "8\n",
      "step :  351 loss :  0.5828284621238708\n",
      "8\n",
      "step :  352 loss :  0.5828284621238708\n",
      "8\n",
      "step :  353 loss :  0.5828284621238708\n",
      "8\n",
      "step :  354 loss :  0.5828284621238708\n",
      "8\n",
      "step :  355 loss :  0.5828284621238708\n",
      "8\n",
      "step :  356 loss :  0.5828284621238708\n",
      "8\n",
      "step :  357 loss :  0.5828284621238708\n",
      "8\n",
      "step :  358 loss :  0.5828284621238708\n",
      "8\n",
      "step :  359 loss :  0.5828284621238708\n",
      "8\n",
      "step :  360 loss :  0.5828284621238708\n",
      "8\n",
      "step :  361 loss :  0.5828284621238708\n",
      "8\n",
      "step :  362 loss :  0.5828284621238708\n",
      "8\n",
      "step :  363 loss :  0.5828284621238708\n",
      "8\n",
      "step :  364 loss :  0.5828284621238708\n",
      "8\n",
      "step :  365 loss :  0.5828284621238708\n",
      "8\n",
      "step :  366 loss :  0.5828284621238708\n",
      "8\n",
      "step :  367 loss :  0.5828284621238708\n",
      "8\n",
      "step :  368 loss :  0.5828284621238708\n",
      "8\n",
      "step :  369 loss :  0.5828284621238708\n",
      "8\n",
      "step :  370 loss :  0.5828284621238708\n",
      "8\n",
      "step :  371 loss :  0.5828284621238708\n",
      "8\n",
      "step :  372 loss :  0.5828284621238708\n",
      "8\n",
      "step :  373 loss :  0.5828284621238708\n",
      "8\n",
      "step :  374 loss :  0.5828284621238708\n",
      "8\n",
      "step :  375 loss :  0.5828284621238708\n",
      "8\n",
      "step :  376 loss :  0.5828284621238708\n",
      "8\n",
      "step :  377 loss :  0.5828284621238708\n",
      "8\n",
      "step :  378 loss :  0.5828284621238708\n",
      "8\n",
      "step :  379 loss :  0.5828284621238708\n",
      "8\n",
      "step :  380 loss :  0.5828284621238708\n",
      "8\n",
      "step :  381 loss :  0.5828284621238708\n",
      "8\n",
      "step :  382 loss :  0.5828284621238708\n",
      "8\n",
      "step :  383 loss :  0.5828284621238708\n",
      "8\n",
      "step :  384 loss :  0.5828284621238708\n",
      "8\n",
      "step :  385 loss :  0.5828284621238708\n",
      "8\n",
      "step :  386 loss :  0.5828284621238708\n",
      "8\n",
      "step :  387 loss :  0.5828284621238708\n",
      "8\n",
      "step :  388 loss :  0.5828284621238708\n",
      "8\n",
      "step :  389 loss :  0.5828284621238708\n",
      "8\n",
      "step :  390 loss :  0.5828284621238708\n",
      "8\n",
      "step :  391 loss :  0.5828284621238708\n",
      "8\n",
      "step :  392 loss :  0.5828284621238708\n",
      "8\n",
      "step :  393 loss :  0.5828284621238708\n",
      "8\n",
      "step :  394 loss :  0.5828284621238708\n",
      "8\n",
      "step :  395 loss :  0.5828284621238708\n",
      "8\n",
      "step :  396 loss :  0.5828284621238708\n",
      "8\n",
      "step :  397 loss :  0.5828284621238708\n",
      "8\n",
      "step :  398 loss :  0.5828284621238708\n",
      "8\n",
      "step :  399 loss :  0.5828284621238708\n",
      "8\n",
      "step :  400 loss :  0.5828284621238708\n",
      "8\n",
      "step :  401 loss :  0.5828284621238708\n",
      "8\n",
      "step :  402 loss :  0.5828284621238708\n",
      "8\n",
      "step :  403 loss :  0.5828284621238708\n",
      "8\n",
      "step :  404 loss :  0.5828284621238708\n",
      "8\n",
      "step :  405 loss :  0.5828284621238708\n",
      "8\n",
      "step :  406 loss :  0.5828284621238708\n",
      "8\n",
      "step :  407 loss :  0.5828284621238708\n",
      "8\n",
      "step :  408 loss :  0.5828284621238708\n",
      "8\n",
      "step :  409 loss :  0.5828284621238708\n",
      "8\n",
      "step :  410 loss :  0.5828284621238708\n",
      "8\n",
      "step :  411 loss :  0.5828284621238708\n",
      "8\n",
      "step :  412 loss :  0.5828284621238708\n",
      "8\n",
      "step :  413 loss :  0.5828284621238708\n",
      "8\n",
      "step :  414 loss :  0.5828284621238708\n",
      "8\n",
      "step :  415 loss :  0.5828284621238708\n",
      "8\n",
      "step :  416 loss :  0.5828284621238708\n",
      "8\n",
      "step :  417 loss :  0.5828284621238708\n",
      "8\n",
      "step :  418 loss :  0.5828284621238708\n",
      "8\n",
      "step :  419 loss :  0.5828284621238708\n",
      "8\n",
      "step :  420 loss :  0.5828284621238708\n",
      "8\n",
      "step :  421 loss :  0.5828284621238708\n",
      "8\n",
      "step :  422 loss :  0.5828284621238708\n",
      "8\n",
      "step :  423 loss :  0.5828284621238708\n",
      "8\n",
      "step :  424 loss :  0.5828284621238708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "step :  425 loss :  0.5828284621238708\n",
      "8\n",
      "step :  426 loss :  0.5828284621238708\n",
      "8\n",
      "step :  427 loss :  0.5828284621238708\n",
      "8\n",
      "step :  428 loss :  0.5828284621238708\n",
      "8\n",
      "step :  429 loss :  0.5828284621238708\n",
      "8\n",
      "step :  430 loss :  0.5828284621238708\n",
      "8\n",
      "step :  431 loss :  0.5828284621238708\n",
      "8\n",
      "step :  432 loss :  0.5828284621238708\n",
      "8\n",
      "step :  433 loss :  0.5828284621238708\n",
      "8\n",
      "step :  434 loss :  0.5828284621238708\n",
      "8\n",
      "step :  435 loss :  0.5828284621238708\n",
      "8\n",
      "step :  436 loss :  0.5828284621238708\n",
      "8\n",
      "step :  437 loss :  0.5828284621238708\n",
      "8\n",
      "step :  438 loss :  0.5828284621238708\n",
      "8\n",
      "step :  439 loss :  0.5828284621238708\n",
      "8\n",
      "step :  440 loss :  0.5828284621238708\n",
      "8\n",
      "step :  441 loss :  0.5828284621238708\n",
      "8\n",
      "step :  442 loss :  0.5828284621238708\n",
      "8\n",
      "step :  443 loss :  0.5828284621238708\n",
      "8\n",
      "step :  444 loss :  0.5828284621238708\n",
      "8\n",
      "step :  445 loss :  0.5828284621238708\n",
      "8\n",
      "step :  446 loss :  0.5828284621238708\n",
      "8\n",
      "step :  447 loss :  0.5828284621238708\n",
      "8\n",
      "step :  448 loss :  0.5828284621238708\n",
      "8\n",
      "step :  449 loss :  0.5828284621238708\n",
      "8\n",
      "step :  450 loss :  0.5828284621238708\n",
      "8\n",
      "step :  451 loss :  0.5828284621238708\n",
      "8\n",
      "step :  452 loss :  0.5828284621238708\n",
      "8\n",
      "step :  453 loss :  0.5828284621238708\n",
      "8\n",
      "step :  454 loss :  0.5828284621238708\n",
      "8\n",
      "step :  455 loss :  0.5828284621238708\n",
      "8\n",
      "step :  456 loss :  0.5828284621238708\n",
      "8\n",
      "step :  457 loss :  0.5828284621238708\n",
      "8\n",
      "step :  458 loss :  0.5828284621238708\n",
      "8\n",
      "step :  459 loss :  0.5828284621238708\n",
      "8\n",
      "step :  460 loss :  0.5828284621238708\n",
      "8\n",
      "step :  461 loss :  0.5828284621238708\n",
      "8\n",
      "step :  462 loss :  0.5828284621238708\n",
      "8\n",
      "step :  463 loss :  0.5828284621238708\n",
      "8\n",
      "step :  464 loss :  0.5828284621238708\n",
      "8\n",
      "step :  465 loss :  0.5828284621238708\n",
      "8\n",
      "step :  466 loss :  0.5828284621238708\n",
      "8\n",
      "step :  467 loss :  0.5828284621238708\n",
      "8\n",
      "step :  468 loss :  0.5828284621238708\n",
      "8\n",
      "step :  469 loss :  0.5828284621238708\n",
      "8\n",
      "step :  470 loss :  0.5828284621238708\n",
      "8\n",
      "step :  471 loss :  0.5828284621238708\n",
      "8\n",
      "step :  472 loss :  0.5828284621238708\n",
      "8\n",
      "step :  473 loss :  0.5828284621238708\n",
      "8\n",
      "step :  474 loss :  0.5828284621238708\n",
      "8\n",
      "step :  475 loss :  0.5828284621238708\n",
      "8\n",
      "step :  476 loss :  0.5828284621238708\n",
      "8\n",
      "step :  477 loss :  0.5828284621238708\n",
      "8\n",
      "step :  478 loss :  0.5828284621238708\n",
      "8\n",
      "step :  479 loss :  0.5828284621238708\n",
      "8\n",
      "step :  480 loss :  0.5828284621238708\n",
      "8\n",
      "step :  481 loss :  0.5828284621238708\n",
      "8\n",
      "step :  482 loss :  0.5828284621238708\n",
      "8\n",
      "step :  483 loss :  0.5828284621238708\n",
      "8\n",
      "step :  484 loss :  0.5828284621238708\n",
      "8\n",
      "step :  485 loss :  0.5828284621238708\n",
      "8\n",
      "step :  486 loss :  0.5828284621238708\n",
      "8\n",
      "step :  487 loss :  0.5828284621238708\n",
      "8\n",
      "step :  488 loss :  0.5828284621238708\n",
      "8\n",
      "step :  489 loss :  0.5828284621238708\n",
      "8\n",
      "step :  490 loss :  0.5828284621238708\n",
      "8\n",
      "step :  491 loss :  0.5828284621238708\n",
      "8\n",
      "step :  492 loss :  0.5828284621238708\n",
      "8\n",
      "step :  493 loss :  0.5828284621238708\n",
      "8\n",
      "step :  494 loss :  0.5828284621238708\n",
      "8\n",
      "step :  495 loss :  0.5828284621238708\n",
      "8\n",
      "step :  496 loss :  0.5828284621238708\n",
      "8\n",
      "step :  497 loss :  0.5828284621238708\n",
      "8\n",
      "step :  498 loss :  0.5828284621238708\n",
      "8\n",
      "step :  499 loss :  0.5828284621238708\n"
     ]
    }
   ],
   "source": [
    "mv_net.train()\n",
    "for t in range(train_episodes):\n",
    "    for b in range(0,len(X),batch_size):\n",
    "        inpt = X[b:b+batch_size,:,:]\n",
    "        target = y[b:b+batch_size]    \n",
    "        \n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32)    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32)\n",
    "    \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "        print(x_batch.size(0))\n",
    "        break\n",
    "    #    lstm_out, _ = mv_net.l_lstm(x_batch,nnet.hidden)    \n",
    "    #    lstm_out.contiguous().view(x_batch.size(0),-1)\n",
    "        output = mv_net(x_batch) \n",
    "        loss = criterion(output.view(-1), y_batch)  \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad() \n",
    "    print('step : ' , t , 'loss : ' , loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  5],\n",
       "        [10, 15],\n",
       "        [20, 25]],\n",
       "\n",
       "       [[10, 15],\n",
       "        [20, 25],\n",
       "        [30, 35]],\n",
       "\n",
       "       [[20, 25],\n",
       "        [30, 35],\n",
       "        [40, 45]],\n",
       "\n",
       "       [[30, 35],\n",
       "        [40, 45],\n",
       "        [50, 55]],\n",
       "\n",
       "       [[40, 45],\n",
       "        [50, 55],\n",
       "        [60, 65]],\n",
       "\n",
       "       [[50, 55],\n",
       "        [60, 65],\n",
       "        [70, 75]],\n",
       "\n",
       "       [[60, 65],\n",
       "        [70, 75],\n",
       "        [80, 85]],\n",
       "\n",
       "       [[70, 75],\n",
       "        [80, 85],\n",
       "        [90, 95]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9b8b298b4fe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmv_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-aa963493f349>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "mv_net(])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [[[10,15],[20,20],[30,35]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = torch.tensor(tmp, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_net.init_hidden(ex.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66.5749]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_net(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10.,  5.],\n",
       "         [20., 10.],\n",
       "         [30., 15.]]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 11],\n",
       "        [11, 12],\n",
       "        [11, 13]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  5],\n",
       "        [10, 15],\n",
       "        [20, 25]],\n",
       "\n",
       "       [[10, 15],\n",
       "        [20, 25],\n",
       "        [30, 35]],\n",
       "\n",
       "       [[20, 25],\n",
       "        [30, 35],\n",
       "        [40, 45]],\n",
       "\n",
       "       [[30, 35],\n",
       "        [40, 45],\n",
       "        [50, 55]],\n",
       "\n",
       "       [[40, 45],\n",
       "        [50, 55],\n",
       "        [60, 65]],\n",
       "\n",
       "       [[50, 55],\n",
       "        [60, 65],\n",
       "        [70, 75]],\n",
       "\n",
       "       [[60, 65],\n",
       "        [70, 75],\n",
       "        [80, 85]],\n",
       "\n",
       "       [[70, 75],\n",
       "        [80, 85],\n",
       "        [90, 95]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = torch.tensor(tmp, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_net.init_hidden(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[66.7826]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_net(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10., 11.],\n",
       "         [11., 12.],\n",
       "         [11., 13.]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
